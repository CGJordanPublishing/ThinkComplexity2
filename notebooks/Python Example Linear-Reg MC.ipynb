{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-fbWjXvFBOu"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Reference: https://ostwalprasad.github.io/machine-learning/Bayesian-Linear-Regression-using-PyMC3.html\n",
        "\n",
        "> In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters.\n",
        "\n",
        "**Linear Regression**\n",
        "\n",
        "\n",
        "In simple linear regression we get point estimates by,\n",
        "$$ y = \\alpha\\ + \\beta\\ *x $$\n",
        " Equation says, there's a linear relationship between variable $x$ and $y$. Slope is controlled by $ \\beta\\ $ and intercept tells about value of $y$ when $x=0$ . Methods like Ordinary Least Squares, optimize the parameters to minimize the error between observed $y$ and predicted $y$. These methods only return single best value for parameters.\n",
        "  \n",
        "** Bayesian Approach: **\n",
        " \n",
        " The same problem can be stated under probablistic framework. We can obtain best values of $ \\alpha\\ $  and $ \\beta\\ $ along with their uncertainity estimations. Probablistically linear regression can be explained as : \n",
        " \n",
        "$$ y \\sim N(\\mu=\\alpha+\\beta x, \\sigma=\\varepsilon) $$\n",
        "\n",
        "$y$ is observed as a Gaussian distribution with mean $ \\mu\\ $ and standard deviation $ \\sigma\\ $. Unlike OLS regression, here it is normally distibuted. Since we do not know the values of $ \\alpha\\ $  , $ \\beta\\ $ and $ \\epsilon\\ $, we have to set prior distributions for them. \n",
        "\n",
        "$$ \\begin{array}{l}{\\alpha \\sim N\\left(\\mu_{\\alpha}, \\sigma_{\\alpha}\\right)} \\\\ {\\beta \\sim N\\left(\\mu_{\\beta}, \\sigma_{\\beta}\\right)} \\\\ {\\varepsilon \\sim U\\left(0, h_{s}\\right)}\\end{array}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this post, I'm going to demonstrate very simple linear regression problem with both OLS and bayesian approach. We will use [PyMC3 package](https://docs.pymc.io/). PyMC3 is a Python package for Bayesian statistical modeling and probabilistic machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp2FvzxGFhDL"
      },
      "source": [
        "###Import basic modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yPCrtJ4kFeij"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBpgJLsVFnei"
      },
      "source": [
        "### Generate linear artificial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AneMX73FFD27"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x - 5 + 2*rng.randn(50)\n",
        "x = x.reshape(-1, 1)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "plt.scatter(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0NQAj-pAv1G"
      },
      "outputs": [],
      "source": [
        "# If you want to install pymc3 in Anaconda, check this: https://anaconda.org/conda-forge/pymc3\n",
        "# conda install -c conda-forge pymc3\n",
        "! pip install pymc3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY0-qOXOF1ih"
      },
      "source": [
        "### Create PyMC3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VepBQWIsFr7i",
        "outputId": "e660e8da-0ee2-4a6c-fc84-0e8493b1f253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the PyMC3 v3.11.5\n"
          ]
        }
      ],
      "source": [
        "import pymc3 as pm\n",
        "print('Running on the PyMC3 v{}'.format(pm.__version__))\n",
        "basic_model =  pm.Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H99_ajdXF88Z"
      },
      "source": [
        "### Define model parameters\n",
        "\n",
        "Context is created for defining model parameters using `with` statement. Using context makes it easy to assign parameters to model.\n",
        "\n",
        "Distributions for  $ \\alpha\\ $  , $ \\beta\\ $ and $ \\epsilon\\ $ are defined. $ \\mu\\ $ is a deterministic variable which calculated using line equation.\n",
        "\n",
        "`Ylikelihood` is a likelihood parameter which is defined ny Normal distribution with $ \\mu\\ $ and $ \\sigma\\ $. Observed values are also passed along with distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q2q_CrzDF50o"
      },
      "outputs": [],
      "source": [
        "with basic_model as bm:\n",
        "\n",
        "    #Priors\n",
        "    alpha = pm.Normal('alpha', mu=0, sd=10)\n",
        "    beta = pm.Normal('beta', mu=0, sd=10)\n",
        "    sigma = pm.HalfNormal('sigma', sd=1)\n",
        "\n",
        "    # Deterministics\n",
        "    mu = alpha + beta*x\n",
        "    \n",
        "    # Likelihood \n",
        "    Ylikelihood = pm.Normal('Ylikelihood', mu=mu, sd=sigma, observed=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A4T1p4bGVtY"
      },
      "source": [
        "### Sampling from posterior\n",
        "\n",
        "As model is defined completely, now we can sample from posterior. PyMC3 automatically chooses appropriate model depending on the type of data. In our case of continuous data, NUTS is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YPxfPEHGTJ4"
      },
      "outputs": [],
      "source": [
        " trace = pm.sample(draws=2000,model=bm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASonWlKNGzbn"
      },
      "source": [
        "### Trace summary and plots\n",
        "Traceplots plots samples histograms and values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrynSbE3GdLJ"
      },
      "outputs": [],
      "source": [
        "pm.traceplot(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfpjhoQWGj3x"
      },
      "outputs": [],
      "source": [
        "print(pm.summary(trace).round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvhLIaXRWyju"
      },
      "source": [
        "### Checking autocorrelation\n",
        "\n",
        "Bar plot of the autocorrelation function for a trace can be plotted using [pymc3.plots.autocorrplot](https://docs.pymc.io/api/plots.html).\n",
        "\n",
        "Autocorrelation dictates the amount of time you have to wait for convergence. If autocorrelation is high, you will have to use a longer burn-in. Low autocorrelation means good exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrwWNpazVvus"
      },
      "outputs": [],
      "source": [
        "pm.autocorrplot(trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_VDZ6oG5PI"
      },
      "source": [
        "### Comparing parameters with Simple Linear Regression (OLS)\n",
        "\n",
        "Parameters can be cross checked using Simple Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBzvdtyHGoRa"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression()\n",
        "ypred =  lm.fit(x,y).predict(x)\n",
        "plt.scatter(x,y)\n",
        "plt.plot(x,ypred)\n",
        "legend_title = 'Simple Linear Regression\\n {} + {}*x'.format(round(lm.intercept_[0],2),round(lm.coef_[0][0],2))\n",
        "legend = plt.legend(loc='upper left', frameon=False, title=legend_title)\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}